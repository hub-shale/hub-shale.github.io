[
  {
    "path": "posts/2022-06-08-bias-in-gender-data-beyond-the-binary/",
    "title": "Bias in Gender Data: Beyond the Binary",
    "description": "A follow-up on the issues underlying inadequate data in a previous post.",
    "author": [
      {
        "name": "Shale Hunter",
        "url": {}
      }
    ],
    "date": "2022-06-08",
    "categories": [],
    "contents": "\nLast December, I wrote this article on identifying grading bias in an undergraduate course I was TAing at the time. Though I did not discover any grading bias in the data, one of the limitations I identified then has stuck with me. I would like to take the time to explore these limitations in more detail in this post.\nWhat’s Missing?\nHere is a summary of the pronouns column from the dataset used in my previous analysis:\n\n\nhide\n\ndata_clean = read.csv(here(\"_posts/2021-12-01-grade-bias-analysis/grading_bias_data_clean.csv\"))\n\nst = data_clean %>% count(pronouns)\n  \ncolnames(st) = c(\"Pronouns\", \"Number of Students\")\nst[1,1] = NA\n\ngt::gt(st)\n\n\n\nPronouns\n      Number of Students\n    NA\n126He/Him/His\n62She/Her/Hers\n38They/Them/Theirs\n3\n\nThere are two clear issues highlighted by this table:\nOnly 3 of the 229 students in the class reported using the pronouns They/Them/Theirs\nMore than half of students in the class did not share their pronouns on their page in UCSB’s student database\nAnyone reading this post as a data scientist will immediately be able to identify the statistical limitations of a dataset with over 50% NA values and variable with a category that is almost completely unrepresented. However in this article I would like to take a deeper look at the underlying causes that forced me to use such an inadequate data source in the first place.\nA Review of Potential Causes for Biased Data in SGM Research\nWhile perhaps not quite yet mainstream, research on Sexual and Gender Minorities (SGM) is a field that is growing rapidly as we are discovering the many ways that identifying with a marginalized group can have harmful effects on a person’s life because of real or assumed discrimination from broader society. This is exceptionally true in the medical field, but certainly is not limited to it.\nAt the same time we are trying to understand and combat the unique challenges faced by SGM individuals and communities, these same people are much more likely to mistrust authorities of all kinds. From doctors advocating for conversion therapy to openly transphobic members of UCSB itself, there is a massive deficit of trust between the LGBTQ+ community and institutional power. Lunn et al. (2019) describes this phenomenon in relation to clinical research:\n“With stigmatizing, discriminatory, or dangerous experiences in society, health care, or investigational communities, potential participant interactions with the health care system and traditional research enterprise may be limited out of fear.”\nThis is known to be true not only in SGM research broadly, but also specifically in relation to college students, the target demographic of my own dataset. Women who have sex with women (WSW) have been shown to have significantly higher rates of eating disorders compared to women who have sex with men (WSM) in the university setting (Von Schell et al. 2018). The same study found that the observed rate of eating disorders in WSW was consistent with the observed rate in existing literature on eating disorders - indicative of the historical exclusion of SGM people in the field.\nEven in my own graduate-level classes at the Bren School, I have noticed a disturbing affinity for the gender binary in teaching examples: while not explicitly discriminatory, it is the kind of unintentional, unexamined microaggression which quietly invalidates nonbinary students’ identity, and with it erodes their trust in the institution our faculty represent.\nEffects of Bias in SGM Data\nWith this combination of a historical lack of effort on researchers’ part to safely and respectfully gather data on minority groups and the ensuing lack of trust between these groups and researchers, it should not be surprising that we can see a clear bias in much of the data that purports to be applicable to SGM people. A review of 43 publicly accessible national, international, and regional data sources on gender and health found that only 14% measured all three dimensions of sexual orientation (identity, behavior, attraction), and no data sources measured transgender-inclusive gender identity (Patterson et al. 2017). Norori et al. (2021) likewise suggest that implicit or explicit biases of research personnel towards SGM individuals can affect data and metadata legitimacy as well as diagnosis accuracy; this is corroborated in Suen et al. (2022), where interviews with SGM focus groups indicated that in-person (as opposed to online or over the phone) is the most uncomfortable, least safe-feeling means of data collection, often because of perceived judgement on the part of the researcher or medical professional.\nThere also remains substantial misunderstanding of SGM people even among those researchers who are trying to ensure that data collection and use is being done in an ethical and responsible way: Norori et al. (2021) describe non-heterosexual women as “generally higher socioeconomic status than heterosexual women” with hardly a thought for the reality underlying whatever data supports this claim. It may be true that a survey of women (or any gender identity) would find a correlation between sexual orientation and socioeconomic status; but treating this data as fact as Norori does is likely a grave mistake. A more accurate phrasing would be: “women who describe themselves as non-heterosexual have higher socioeconomic status” - where the likely explanation is that wealthy and highly educated individuals have the privilege of expressing their sexual or gender identity openly in a way that less wealthy or educated individuals may not have because of familial, religious, or other practical restrictions on their ability to live openly (which extends to medical surveys). An extreme version of this is children under 18, most of whom report that they wouldn’t participate in SGM research or surveys at all if it required parental consent (Macapagal et al. 2016).\nAll this is to say that the problem encountered in my data is not unique - but how can we as researchers ensure that the data we do collect is unbiased, complete, and respectful of its contributors?\nWays to Combat Bias in SGM Research\nSeveral approaches have been taken to try and improve SGM data from an ethics and bias perspective. Interviews and focus groups like those described in Suen et al. (2022) offer a means for SGM individuals to express their own concerns directly to the organizers of future research, with the goal of modifying methods to reflect the unique needs of the population. This also helps avoid the problem of non-SGM research personnel making the mistake of thinking they know best when it comes to what makes someone with a marginalized identity feel more or less comfortable in a research setting.\nAnother approach is described in Lunn et al. (2019): the PRIDE Study (Population Research in Identity and Disparities for Equality) is a longitudinal SGM research project based out of Stanford that takes advantage of the well-documented preference in the SGM community for remote interaction with researchers by developing a mobile application to complete surveys and give feedback on research topics and methodologies. Using this approach allowed 16,394 SGM respondents (98% sexual minority, 15% gender minority) from a range of geographic and demographic groups in the US to safely and comfortably provide 3,544 suggested important SGM health topics, complete 24,022 surveys, and use forum and voting features to share opinions on the methods and functionality of the app itself.\nFinally, our responsibility in relation to the biases discovered in SGM data is not just as researchers, but also as educators. The data science and R communities have made progress in addressing other types of latent injustices through the replacement of problematic datasets with better ones. There is certainly room for such a solution in SGM data education: with the limitations of current data sources clearly highlighted by reviews such as Patterson et al. (2017), new teaching data files with complete gender information would be an invaluable tool both in service of the demarginalization of diverse gender identities and education of the broader population.\nFinal Thoughts\nIn the end, the only way to ensure researchers are collecting unbiased and complete data about our SGM community is by ensuring that the community feels safe providing their data as well as that the data is collected in a way that benefits that community. More than 25% of SGM respondents to one survey said of this kind of research: “I don’t need anything in return. I will keep participating to improve LGBTQ health” (Lunn et al. 2019).\nEven so, we have a long way to go before this is broadly achieved in the US, let alone the world. Even in a highly educated liberal institution like USCB there remain barriers to unbiased SGM data collection, as evidenced by my own experience with data collection:\n\n\nhide\n\ngt::gt(st)\n\n\n\nPronouns\n      Number of Students\n    NA\n126He/Him/His\n62She/Her/Hers\n38They/Them/Theirs\n3\n\nEven here there appears to be some level of apathy or lack of respect for the importance of gender identity in the student body as a whole. And it remains difficult to ascertain exactly how much peer, faculty, or institutional judgement is affecting SGM individuals’ willingness to share their identity on school platforms such as the one I used to collect data for my project last winter. Spizzirri et al. (2021) report global estimations of gender-diverse individuals between 0.1 to 2% of the population, which my proportion of 3/229 students (1.3%) would be consistent with. However, Lunn et al. (2019) report over 15% gender minority in their sample - of course this is a sample that is specifically targeting the LGBTQ+ community and would be expected to be greater than the overall population. But there remains a concern that the true proportion is higher than the 2% upper bound of observed gender minorities because of real and persistent stigmas relating to non-standard gender identities.\nRegardless of what the true proportion is, it is important that research methods take into account the many ways in which SGM populations have been mistreated in the past, and how that mistreatment affects current relationships between data collector and data contributor. Hopefully then I will be able to really get a complete and unbiased estimation of my objectivity as a grader!\nReferences\nLunn, M. R., Capriotti, M. R., Flentje, A., Bibbins-Domingo, K., Pletcher, M. J., Triano, A. J., Sooksaman, C., Frazier, J., & Obedin-Maliver, J. (2019). Using mobile technology to engage sexual and gender minorities in clinical research. PLoS ONE, 14(5), e0216282–e0216282. https://doi.org/10.1371/journal.pone.0216282\nMacapagal, K., Coventry, R., Arbeit, M. R., Fisher, C. B., & Mustanski, B. (2016). “I Won’t Out Myself Just to Do a Survey”: Sexual and Gender Minority Adolescents’ Perspectives on the Risks and Benefits of Sex Research. Archives of Sexual Behavior, 46(5), 1393–1409. https://doi.org/10.1007/s10508-016-0784-5\nMay, A., Wachs, J., & Hannák, A. (2019). Gender differences in participation and reward on Stack Overflow. Empirical Software Engineering : An International Journal, 24(4), 1997–2019. https://doi.org/10.1007/s10664-019-09685-x\nNorori, N., Hu, Q., Aellen, F. M., Faraci, F. D., & Tzovara, A. (2021). Addressing bias in big data and AI for health care: A call for open science. Patterns (New York, N.Y.), 2(10), 100347–100347. https://doi.org/10.1016/j.patter.2021.100347\nPatterson, J. G., Jabson, J. M., & Bowen, D. J. (2017). Measuring Sexual and Gender Minority Populations in Health Surveillance. LGBT Health, 4(2), 82–105. https://doi.org/10.1089/lgbt.2016.0026\nSpizzirri, G., Eufrásio, R., Lima, M.C.P. et al. Proportion of people identified as transgender and non-binary gender in Brazil. Sci Rep 11, 2240 (2021). https://doi.org/10.1038/s41598-021-81411-4\nSuen, L. W., Lunn, M. R., Sevelius, J. M., Flentje, A., Capriotti, M. R., Lubensky, M. E., Hunt, C., Weber, S., Bahati, M., Rescate, A., Dastur, Z., & Obedin-Maliver, J. (2022). Do Ask, Tell, and Show: Contextual Factors Affecting Sexual Orientation and Gender Identity Disclosure for Sexual and Gender Minority People. LGBT Health, 9(2), 73–80. https://doi.org/10.1089/lgbt.2021.0159\nVon Schell, A., Ohrt, T. K., Bruening, A. B., Perez, M. (2018). Rates of disordered eating behaviors across sexual minority undergraduate men and women. Psychology of Sexual Orientation and Gender Diversity, 5(3), 352–359. https://doi.org/10.1037/sgd0000278\n\n\n\n",
    "preview": {},
    "last_modified": "2022-06-09T23:26:25-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-01-grade-bias-analysis/",
    "title": "Grade Bias Analysis",
    "description": "A statistical analysis of grading bias for TAs in a UCSB Art History course.",
    "author": [
      {
        "name": "Shale Hunter",
        "url": {}
      }
    ],
    "date": "2021-12-02",
    "categories": [],
    "contents": "\n\nContents\nStatistical analysis of grading biases\nIntroduction\nTidying the data\nVerifying basic assumptions of normality\nAutocorrelation\nGender Bias\nConclusion\n\n\nStatistical analysis of grading biases\nIntroduction\nThe basic idea for this little project came to me when I was grading essays a few months ago for the class I am TAing this fall: as I methodically plodded through page after page of art history interpretation of dubious quality, I began to wonder if I were really treating each essay with equal care. By the time I graded my 50th and final essay, I was sure that I was not.\nWhile I wasn’t particularly happy with my intuitive conclusion that I probably wasn’t being the fairest possible grader for my students, I was also a very busy masters student who wasn’t going to spend an extra \\(15 min/essay * 50 essays= 12.5 hours\\) to ensure that I was being 100% objective in my grading1. That said, the issue continued to linger in the back of my mind. So, when I was presented with the mandatory opportunity to conduct a self-driven statistical analysis for my Statistics for Environmental Data Science class, I jumped at the opportunity to tackle a question that had been nagging at me for several weeks, now secure in the knowledge that my curiosity would now be counted as progress towards my degree.\nWhat follows is a statistical exploration of possible grading biases in 5 Teaching Assistants (including myself) and their 229 students who actually turned in essays.\nTidying the data\nAs TA, I have access to a whole lot of confidential student data which I probably shouldn’t be sharing via a public blog post for any old internet traveler to discover and exploit. So I began by cleaning up the raw data both in order to protect student’s privacy and to remove any unusable or missing data.\nThe cleaned dataset is available by request for anyone who wants to recreate the results of my actual statistical analysis with data that maintains the privacy of my students.\n\n\nhide\n\ndata_clean = read.csv(\"grading_bias_data_clean.csv\")\n\n\n\nThe basic components of the cleaned dataset are as follows:\n- name: Student name (last, first initial), partially anonymized\n- pronouns: Self-reported student pronouns (not present for all students)\n- TA: Teaching Assistant (Surname) and section, used for ordering students\n- essay: essay grade, %\n- quizzes_pct: cumulative quiz grade, %\n- participation_pct: cumulative participation grade, % (not used in this analysis because some TAs had not finished entering participation grades at the time data was accessed)\n\n\nhide\n\ngt(head(data_clean, n=3))\n\n\n\nname\n      pronouns\n      TA\n      essay\n      quizzes_pct\n      participation_pct\n    Barrientos C\nHe/Him/His\nHu]_10\n85\n60.19\n0Betova A\nShe/Her/Hers\nHu]_10\n88\n78.70\n0Bhaduri A\n\nHu]_10\n88\n86.11\n0\n\nVerifying basic assumptions of normality\nBefore getting started with answering my question, I’ll take a quick look at the data to make sure it satisfies some basic requirements for normality. The basic histograms below show that essays (A) have a reasonably normal distribution (with one obvious low outlier). Quizzes grades (C), on the other hand, look like a log distribution might fit better - unfortunately, the log of quiz grades (D) also looks like it would fit a log normal curve (and so does the log of the log). So at this point I decided to just roll with the original quiz data with the recognition that any standard errors retrieved from this analysis will be effected, but bias should remain unaffected. Likewise, the pronouns data (B), which will be used in a follow-up analysis, have some clear limitations because of the uneven distribution of students who have listed their pronouns, particularly nonbinary students.\n\n\nhide\n\nessays = ggplot(data_clean, aes(x=essay)) + geom_histogram(bins = 40) + xlab(\"essay grade\")\n\npronoun = data_clean %>% \n  count(pronouns) %>% \n  ggplot(aes(x=pronouns, y=n)) + geom_col() +\n  geom_text(aes(label=n), position=position_dodge(width=0.9), vjust=-0.25) + theme(axis.text.x = element_text(angle = 10))\n\n# quizzes stuff:\n\nquiz = ggplot(data_clean, aes(x=quizzes_pct)) + geom_histogram() + xlab(\"quiz grade\")\n\nlog_qz = data_clean %>% \n  mutate(log_quiz = log(quizzes_pct))\nlog_quizzes = ggplot(log_qz, aes(x=log_quiz)) + geom_histogram() + xlab(\"log(quiz grade)\")\n\nggarrange(essays, pronoun, quiz, log_quizzes,\n          labels = c(\"A\", \"B\", \"C\", \"D\"),\n          ncol = 2, nrow = 2)\n\n\n\n\nAutocorrelation\nThe goal of this analysis is to see if the team of Teaching Assistants (myself included) has been grading in a biased way. Specifically, because we/I graded the essay assignment in alphabetical order, it is possible to track if I graded a student’s essay “in response” to the previous student’s essay instead of based on the essay’s individual merit. Hypothetically, this could take two forms: 1) differential grading, in which an essay is graded higher than it ought to be because the essay before was of particularly poor quality, or and essay is graded lower than it ought to be because the essay before was of particularly high quality. Or 2) uniform grading, in which an essay is graded higher/lower than it ought to be because it isn’t all that different from the previous essay, and takes on a grade similar/identical to that of the previous essay. The null hypothesis here is that there is no effect by the previous essay’s grade on the current essay, and therefore no grading bias (yay!).\nTo test this, we can use the acf() function, which compares each essay grade to each of the previous essay grades in data_clean up to lag.max. In this first example, we simply use a lag of 1 to compare each essay to the one immediately preceding it.\n\n\nhide\n\nbias_acf = acf(data_clean$essay, lag.max = 1)\n\n\n\n\nFigure 1: ACF plot with a lag of 1, no significant results.\n\n\n\nAs the autocorrelation plot above shows, there is no relationship between an essay’s grade and its predecessor (as we would hope to see!). The dotted blue lines show the cutoff for statistical significance at a 95% confidence interval: because the autocorrelation value at lag = 1 is between 0 and the blue line (0.0167293 to be precise), it is statistically insignificant.\nWhile my initial hypothesis was that essays graded immediately next to each other might be correlated, the autocorrelation performed above failed to reject the null hypothesis that there was zero correlation between grades. This is good news, but there are other ways we can use acf() to see if grading bias was introduced another way.\nNext, we can use a larger lag in order to see if an essay’s grade is correlated with grades of essays farther away on the roster:\n\n\nhide\n\nbias_acf10 = acf(data_clean$essay, lag.max = 10)\n\n\n\n\nFigure 2: ACF plot with lag of 10, significant results at lag = 4, 7 (indicated by passing the dotted blue line).\n\n\n\nInterestingly, this autocorrelation plot with a maximum lag of 10 shows that there are two points where an essay’s grade is correlated with other essay’s grades such that the acf value rises above the cutoff for statistical significance: t-4 and t-7.\nWhile these results are unexpected, there are several possible explanations for why this might be the case: firstly, it is a possibility that all TAs graded essays in groups of 3-4 in order to avoid getting tired of grading and thereby giving lower grades to essays graded later on. If each grader came back from each break feeling more refreshed, happier, and more likely to give a better grade on an essay, then a pattern such as the one above might appear. However, it is important to note that at no point is there ever a significant negative autocorrelation, which seems to argue against a pattern like this - if an essay’s placement at the beginning of such a group lead to higher grades, then another essay’s placement at the end of the group should likewise indicate lower grades, and a negative acf value. But, that is clearly not the case in this data.\nAn alternative explanation is that TAs may be consciously or unconsciously tracking the numbers of A’s, B’s, or C’s they have given, and compensating for higher or lower grades every few essays. For example, if a TA hadn’t given any A’s for several essays in a row, they might give the next essay an A in order to make up for a perceived deficit (even if the essay in question only really deserved a B+).\nIt should be noted that I ran the acf() function with a lag as high as 50 and no other significant correlations were found, so even if either of these biases were real the periodicity does not extend beyond a lag of 7.\nGender Bias\nBeyond my initial curiosity regarding the possibility of an autocorrelation bias in my grading, I was also recently reminded of the potential for gender bias in grading. So this next section will take a look at the subsection of students who have reported gender information on the class portal in order to identify a possible systemic devaluation of academic work based on gender.\nFor this test, we first need to restrict our observations to only those students who have provided pronouns.\n\n\nhide\n\ndata_gendered = data_clean %>% \n  filter(pronouns != \"\")\n\n\n\nThen, we can conduct a simple linear regression with the lm() function to see if there is an effect of gender on essay grades:\n\n\nhide\n\ngender_model_essay = lm(essay ~ pronouns + quizzes_pct, data = data_gendered)\ngender_summ = summary(gender_model_essay)\n\n\n\nInterestingly, the model gives a positive coefficient for women (1.2580465) but a negative coefficient for nonbinary (-2.5374933) in relationship to a male default. This means that the model expects women’s essay grades to be higher than men’s, but nonbinary students’ essays grades to be lower.\nThere are two major caveats to these results: firstly, neither of the gender coefficients had p-values even close to the traditional cutoff of 0.05 (she/her: 0.2728258, they/them: 0.4332378). Furthermore, with only 3 students listing their pronouns as They/Them/Theirs, it is particularly difficult to make stastically sound assumptions about this set of students even in relation to the rest of this dataset.\nAlthough none of the gender coefficients are significant (indicating a lack of any conclusive grading bias based on gender), the quizzes_pct coefficient of 0.1111361, which indicates that for every one percent increase in a student’s overall quiz grade their essay grade is expected to go up by 0.1111361 percentage points, is statistically significant at the 0.05 level given a p-value of 0.0118374. This validates my choice of quiz grade as a good benchmark against which we can test for bias, because it shows that a student’s own grades are correlated with each other (as they should be).\nAll of this can be visualized with the plot below:\n\n\nhide\n\nggplot(data = data_gendered, aes(x=quizzes_pct, y=essay, color=pronouns)) +\n  geom_jitter() +\n  geom_line(data = augment(gender_model_essay), aes(y = .fitted)) +\n  labs(title = \"Effect of Gender on Essay Grade Using Quiz Grade as Baseline\") +\n  xlab(\"Quiz Grade (%)\") +\n  ylab(\"Essay Grade (%)\") +\n  theme_bw()\n\n\n\n\nFigure 3: Essay grades are marginally higher for women and lower for nonbinary students as compared to men, but not statistically significant.\n\n\n\nVisualizing the model with the plot above makes it clear that there is a positive relationship between quiz grade and essay grade, as indicated by the positive and statistically significant quizzes_pct coefficient in the model. Likewise, though the offset of the lines for different genders shows women with overall higher grades and nonbinary students with overall lower grades than men, the large amount of spread in all the data points is a visual indicator that these results are not statistically significant.\nConclusion\nAfter conducting this analysis, I can more or less rest easy knowing that I am as honest a grader as can be expected. A linear model showed that essay grades were correlated with the same student’s quiz grades, but not significantly correlated with gender. Similarly, a test of autocorrelation showed that the grade of the essay immediately beforehand has no effect on an essay’s grade, which was my original concern. And while two essay lags were found to have significant correlation, this can be attributed to the type of human failings which are all too common, and are only being properly understood as we gather and interpret increasingly large amounts of data on the topic.\n\nAfter all, its an art history class, and an essay to boot - how much true objectivity is really expected here?↩︎\n",
    "preview": "posts/2021-12-01-grade-bias-analysis/grade_bias_analysis_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-12-02T22:15:37-07:00",
    "input_file": {}
  }
]
